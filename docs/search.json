[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "temp_website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Matthew Tan",
    "section": "About Me",
    "text": "About Me\nI‚Äôm a Year 2 Applied Computing (FinTech) student at Singapore Institute of Technology with a passion for leveraging technology to create meaningful impact in financial services and technology sectors.\nWith hands-on experience in full-stack development, database architecture, and AI integration, I‚Äôve worked across various domains including cloud computing, AI voice automation, and FinTech solutions. My technical toolkit includes Java, Python, JavaScript, SQL, and modern web technologies, complemented by a strong foundation in business finance and data analytics.\n\nWhat I Do\n\nFull-Stack Development: Building robust web applications with modern frameworks\nAI Integration: Developing intelligent systems with LLM and machine learning\nCloud Computing: Architecting scalable solutions on AWS\nFinTech Solutions: Creating innovative financial technology applications\nTeaching: Currently serving as a Teaching Assistant for INF1002 Programming Fundamentals\n\n\n\nCurrent Focus\nI‚Äôm currently aiming to pass my interview at Prudential on friday :)\n\nContact: mailformatthewtan@gmail.com | LinkedIn"
  },
  {
    "objectID": "reflection.html",
    "href": "reflection.html",
    "title": "Reflection on the Use of GenAI",
    "section": "",
    "text": "For this assessment, I primarily used Claude by Anthropic as my GenAI tool. I chose Claude for several reasons:\n\nTechnical Depth: Claude excels at understanding complex technical requirements and providing detailed, accurate code\nContext Retention: Its ability to maintain context throughout long conversations helped streamline the development process\nCode Quality: Claude generates clean, well-documented code that follows best practices\nExplanation Quality: Beyond just providing solutions, Claude explains the ‚Äúwhy‚Äù behind technical decisions\n\n\n\n\n\n\nChallenge: I encountered issues with GitHub authentication and repository setup.\nHow Claude Helped: - Provided step-by-step commands for Git configuration - Helped troubleshoot 2FA authentication issues - Guided me through the process of making my repository public and configuring GitHub Pages\nExample: When I couldn‚Äôt access my school GitHub due to 2FA issues, Claude helped me switch to my personal account and configure Git with the correct credentials:\ngit config --global user.name \"itsmatthewtan\"\ngit config --global user.email \"mailformatthewtan@gmail.com\"\n\n\n\nChallenge: Setting up and configuring Quarto for GitHub Pages deployment.\nHow Claude Helped: - Explained the importance of the output-dir: docs configuration - Guided me through the rendering process in RStudio - Helped troubleshoot when files weren‚Äôt appearing in the correct directory\nKey Learning: Understanding that I needed to render from the correct project directory, not the temporary website folder.\n\n\n\nChallenge: Structuring content effectively using Markdown and creating professional-looking pages.\nHow Claude Helped: - Generated well-formatted Markdown templates for all pages - Helped organize my existing content (resume, project descriptions) into web-friendly formats - Provided examples of effective Markdown syntax for images, links, and code blocks\nExample: Claude transformed my resume bullet points into engaging narrative content for the FYP project page, making it more suitable for a personal website.\n\n\n\nChallenge: Various technical issues throughout the development process.\nHow Claude Helped: - Diagnosed why files weren‚Äôt being pushed to GitHub - Explained the Git staging process when I was confused about unstaged files - Provided clear explanations for each error message I encountered\n\n\n\n\n\n\n\nReduced research time significantly\nGot instant answers to technical questions\nAvoided trial-and-error debugging\n\n\n\n\n\nReceived explanations alongside solutions\nLearned best practices in real-time\nUnderstood the ‚Äúwhy‚Äù behind technical decisions\n\n\n\n\n\nCaught potential mistakes before they became problems\nReceived warnings about common pitfalls\nGot guidance on proper workflows\n\n\n\n\n\nGenerated clean, well-structured code\nProduced professional-looking documentation\nCreated consistent formatting across all pages\n\n\n\n\n\n\n\n\nSometimes needed to repeat information when conversations got long\nHad to be specific about which project or folder I was working in\nRequired clear communication to avoid misunderstandings\n\n\n\n\n\nClaude couldn‚Äôt directly see my file structure\nNeeded to provide screenshots to show current state\nHad to manually verify that suggested commands worked\n\n\n\n\n\nOccasionally made assumptions about my setup\nNeeded to verify that paths and configurations were correct for my system\nHad to adapt some suggestions to my specific environment\n\n\n\n\n\nEasy to ask for solutions without fully understanding them\nImportant to review and understand all generated code\nNeed to maintain critical thinking about suggestions\n\n\n\n\n\n\n\n\nProgramming Assignments: Quick debugging and code review partner\nResearch Projects: Help with data analysis and visualization code\nDocumentation: Generate clear, professional documentation\nLearning New Technologies: Accelerate learning curve for new frameworks\n\n\n\n\n\nCode Reviews: Get instant feedback on code quality\nProblem Solving: Brainstorm solutions to complex technical challenges\nDocumentation: Create comprehensive technical documentation\nRapid Prototyping: Quickly build proof-of-concepts\n\n\n\n\n\nAlways Verify: Never blindly copy-paste without understanding\nAsk for Explanations: Request the reasoning behind solutions\nIterate and Refine: Use GenAI as a collaborative tool, not a replacement for thinking\nLearn Actively: Use GenAI responses as learning opportunities\n\n\n\n\n\nUsing Claude for this assessment has been invaluable. It‚Äôs not just about getting quick answers‚Äîit‚Äôs about having an intelligent assistant that can explain concepts, catch mistakes, and help me work more efficiently. The key is to use it as a learning tool rather than a crutch.\nThe most important lesson: GenAI is most effective when you engage with it critically, ask good questions, and take time to understand the solutions it provides rather than just implementing them blindly.\nMoving forward, I plan to continue using GenAI tools like Claude as a ‚Äúsenior developer‚Äù I can consult, while always maintaining my own understanding and critical thinking about the code and solutions I implement.\n\nThis reflection represents my genuine experience using Claude (Anthropic) to complete this Quarto website assessment, including both the benefits and challenges I encountered."
  },
  {
    "objectID": "reflection.html#my-experience-with-claude-anthropic",
    "href": "reflection.html#my-experience-with-claude-anthropic",
    "title": "Reflection on the Use of GenAI",
    "section": "",
    "text": "For this assessment, I primarily used Claude by Anthropic as my GenAI tool. I chose Claude for several reasons:\n\nTechnical Depth: Claude excels at understanding complex technical requirements and providing detailed, accurate code\nContext Retention: Its ability to maintain context throughout long conversations helped streamline the development process\nCode Quality: Claude generates clean, well-documented code that follows best practices\nExplanation Quality: Beyond just providing solutions, Claude explains the ‚Äúwhy‚Äù behind technical decisions\n\n\n\n\n\n\nChallenge: I encountered issues with GitHub authentication and repository setup.\nHow Claude Helped: - Provided step-by-step commands for Git configuration - Helped troubleshoot 2FA authentication issues - Guided me through the process of making my repository public and configuring GitHub Pages\nExample: When I couldn‚Äôt access my school GitHub due to 2FA issues, Claude helped me switch to my personal account and configure Git with the correct credentials:\ngit config --global user.name \"itsmatthewtan\"\ngit config --global user.email \"mailformatthewtan@gmail.com\"\n\n\n\nChallenge: Setting up and configuring Quarto for GitHub Pages deployment.\nHow Claude Helped: - Explained the importance of the output-dir: docs configuration - Guided me through the rendering process in RStudio - Helped troubleshoot when files weren‚Äôt appearing in the correct directory\nKey Learning: Understanding that I needed to render from the correct project directory, not the temporary website folder.\n\n\n\nChallenge: Structuring content effectively using Markdown and creating professional-looking pages.\nHow Claude Helped: - Generated well-formatted Markdown templates for all pages - Helped organize my existing content (resume, project descriptions) into web-friendly formats - Provided examples of effective Markdown syntax for images, links, and code blocks\nExample: Claude transformed my resume bullet points into engaging narrative content for the FYP project page, making it more suitable for a personal website.\n\n\n\nChallenge: Various technical issues throughout the development process.\nHow Claude Helped: - Diagnosed why files weren‚Äôt being pushed to GitHub - Explained the Git staging process when I was confused about unstaged files - Provided clear explanations for each error message I encountered\n\n\n\n\n\n\n\nReduced research time significantly\nGot instant answers to technical questions\nAvoided trial-and-error debugging\n\n\n\n\n\nReceived explanations alongside solutions\nLearned best practices in real-time\nUnderstood the ‚Äúwhy‚Äù behind technical decisions\n\n\n\n\n\nCaught potential mistakes before they became problems\nReceived warnings about common pitfalls\nGot guidance on proper workflows\n\n\n\n\n\nGenerated clean, well-structured code\nProduced professional-looking documentation\nCreated consistent formatting across all pages\n\n\n\n\n\n\n\n\nSometimes needed to repeat information when conversations got long\nHad to be specific about which project or folder I was working in\nRequired clear communication to avoid misunderstandings\n\n\n\n\n\nClaude couldn‚Äôt directly see my file structure\nNeeded to provide screenshots to show current state\nHad to manually verify that suggested commands worked\n\n\n\n\n\nOccasionally made assumptions about my setup\nNeeded to verify that paths and configurations were correct for my system\nHad to adapt some suggestions to my specific environment\n\n\n\n\n\nEasy to ask for solutions without fully understanding them\nImportant to review and understand all generated code\nNeed to maintain critical thinking about suggestions\n\n\n\n\n\n\n\n\nProgramming Assignments: Quick debugging and code review partner\nResearch Projects: Help with data analysis and visualization code\nDocumentation: Generate clear, professional documentation\nLearning New Technologies: Accelerate learning curve for new frameworks\n\n\n\n\n\nCode Reviews: Get instant feedback on code quality\nProblem Solving: Brainstorm solutions to complex technical challenges\nDocumentation: Create comprehensive technical documentation\nRapid Prototyping: Quickly build proof-of-concepts\n\n\n\n\n\nAlways Verify: Never blindly copy-paste without understanding\nAsk for Explanations: Request the reasoning behind solutions\nIterate and Refine: Use GenAI as a collaborative tool, not a replacement for thinking\nLearn Actively: Use GenAI responses as learning opportunities\n\n\n\n\n\nUsing Claude for this assessment has been invaluable. It‚Äôs not just about getting quick answers‚Äîit‚Äôs about having an intelligent assistant that can explain concepts, catch mistakes, and help me work more efficiently. The key is to use it as a learning tool rather than a crutch.\nThe most important lesson: GenAI is most effective when you engage with it critically, ask good questions, and take time to understand the solutions it provides rather than just implementing them blindly.\nMoving forward, I plan to continue using GenAI tools like Claude as a ‚Äúsenior developer‚Äù I can consult, while always maintaining my own understanding and critical thinking about the code and solutions I implement.\n\nThis reflection represents my genuine experience using Claude (Anthropic) to complete this Quarto website assessment, including both the benefits and challenges I encountered."
  },
  {
    "objectID": "fyp.html",
    "href": "fyp.html",
    "title": "FYP Project: Meta-Data Management & Intelligent Search Platform",
    "section": "",
    "text": "During my internship at First Com Solutions Pte Ltd (Aug 2021 - Mar 2022), I led the development of an intelligent metadata management and search platform as part of the company‚Äôs digital transformation initiative.\n\n\nOrganizations today struggle with retrieving relevant information from massive datasets. Traditional search methods often fail to provide accurate, ranked results that users actually need. Our goal was to build a system that could:\n\nEfficiently index and retrieve large datasets\nIntelligently rank search results based on relevance\nProvide smart recommendations using machine learning\nScale to handle enterprise-level data volumes\n\n\n\n\n\n\nThe platform was built using a data-driven approach with the following components:\nBackend Infrastructure - Structured SQL databases for metadata storage - Custom indexing algorithms for fast retrieval - RESTful API for client-server communication\nMachine Learning Pipeline - Integrated ML algorithms for intelligent ranking - Recommendation engine based on user behavior patterns - Natural language processing for query understanding\nSearch Engine Features - Advanced indexing mechanisms - Relevance scoring algorithms - Personalized search result ranking - Auto-complete and query suggestions\n\n\n\n\n\nIntelligent Metadata Discovery: Automatically categorizes and tags data entries for improved searchability\nRanked Search Results: Uses ML models to rank results based on relevance and user context\nSmart Recommendations: Suggests related metadata entries based on search patterns\nScalable Architecture: Designed to handle enterprise-level datasets efficiently\n\n\n\n\n\nLanguages: Python, SQL, JavaScript\nFrameworks: Custom search engine implementation\nDatabase: Structured SQL database\nML Libraries: Scikit-learn, TensorFlow\nTools: Git version control, Power BI for analytics\n\n\n\n\nThe platform significantly improved the company‚Äôs data discovery capabilities:\n\nReduced average search time by implementing efficient indexing\nImproved search accuracy through ML-powered ranking\nEnhanced user satisfaction with personalized recommendations\nContributed to the company‚Äôs digital transformation initiatives\n\n\n\n\nThis project taught me:\n\nThe importance of scalable database design for large datasets\nHow to implement and tune machine learning algorithms for production use\nThe value of user-centered design in search interfaces\nHow to balance performance with accuracy in real-time systems\n\n\n\n\nIf I were to continue this project, I would explore:\n\nIntegration with vector databases for semantic search\nImplementation of large language models for natural language queries\nReal-time data synchronization and updates\nAdvanced analytics dashboard for search pattern insights\n\n\nThis project was completed as part of my Final Year Project during my internship at First Com Solutions Pte Ltd, where I worked as a Web Developer Intern and contributed to the company‚Äôs AI and digital transformation initiatives."
  },
  {
    "objectID": "fyp.html#project-overview",
    "href": "fyp.html#project-overview",
    "title": "FYP Project: Meta-Data Management & Intelligent Search Platform",
    "section": "",
    "text": "During my internship at First Com Solutions Pte Ltd (Aug 2021 - Mar 2022), I led the development of an intelligent metadata management and search platform as part of the company‚Äôs digital transformation initiative.\n\n\nOrganizations today struggle with retrieving relevant information from massive datasets. Traditional search methods often fail to provide accurate, ranked results that users actually need. Our goal was to build a system that could:\n\nEfficiently index and retrieve large datasets\nIntelligently rank search results based on relevance\nProvide smart recommendations using machine learning\nScale to handle enterprise-level data volumes\n\n\n\n\n\n\nThe platform was built using a data-driven approach with the following components:\nBackend Infrastructure - Structured SQL databases for metadata storage - Custom indexing algorithms for fast retrieval - RESTful API for client-server communication\nMachine Learning Pipeline - Integrated ML algorithms for intelligent ranking - Recommendation engine based on user behavior patterns - Natural language processing for query understanding\nSearch Engine Features - Advanced indexing mechanisms - Relevance scoring algorithms - Personalized search result ranking - Auto-complete and query suggestions\n\n\n\n\n\nIntelligent Metadata Discovery: Automatically categorizes and tags data entries for improved searchability\nRanked Search Results: Uses ML models to rank results based on relevance and user context\nSmart Recommendations: Suggests related metadata entries based on search patterns\nScalable Architecture: Designed to handle enterprise-level datasets efficiently\n\n\n\n\n\nLanguages: Python, SQL, JavaScript\nFrameworks: Custom search engine implementation\nDatabase: Structured SQL database\nML Libraries: Scikit-learn, TensorFlow\nTools: Git version control, Power BI for analytics\n\n\n\n\nThe platform significantly improved the company‚Äôs data discovery capabilities:\n\nReduced average search time by implementing efficient indexing\nImproved search accuracy through ML-powered ranking\nEnhanced user satisfaction with personalized recommendations\nContributed to the company‚Äôs digital transformation initiatives\n\n\n\n\nThis project taught me:\n\nThe importance of scalable database design for large datasets\nHow to implement and tune machine learning algorithms for production use\nThe value of user-centered design in search interfaces\nHow to balance performance with accuracy in real-time systems\n\n\n\n\nIf I were to continue this project, I would explore:\n\nIntegration with vector databases for semantic search\nImplementation of large language models for natural language queries\nReal-time data synchronization and updates\nAdvanced analytics dashboard for search pattern insights\n\n\nThis project was completed as part of my Final Year Project during my internship at First Com Solutions Pte Ltd, where I worked as a Web Developer Intern and contributed to the company‚Äôs AI and digital transformation initiatives."
  },
  {
    "objectID": "data-visualization.html",
    "href": "data-visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "During my Data Analytics module in Year 1, I conducted an analysis of cryptocurrency trading patterns among Singapore investors. This project aimed to identify trends in trading behavior, popular cryptocurrencies, and correlations between market volatility and trading volume.\n\n\n\nDataset Details: - Source: Simulated trading data based on public cryptocurrency APIs (CoinGecko, Binance) - Variables: Trading volume, cryptocurrency type, time of day, price volatility, trader demographics - Time Period: January 2023 - December 2024 (24 months) - Size: 50,000+ transaction records from 1,200 simulated traders - Metrics: BTC, ETH, SOL, MATIC trading volumes, hourly price changes, trader age groups\n\n\n\n\n\n\nCryptocurrency Trading Patterns\n\n\nNote: This is a multi-panel visualization showing: - Top Left: Trading volume by hour of day (shows peak activity at 9-10 PM SGT) - Top Right: Most traded cryptocurrencies (Bitcoin dominates at 45%, followed by Ethereum at 30%) - Bottom Left: Correlation heatmap between volatility and trading volume - Bottom Right: Age group distribution of active traders\n\n\n\nKey Insights:\n\nPeak Trading Hours: Singapore crypto traders are most active between 9-11 PM, coinciding with evening relaxation time after work\nCryptocurrency Preferences:\n\nBitcoin: 45% of trading volume\nEthereum: 30% of trading volume\nSolana: 15% of trading volume\nOther altcoins: 10% of trading volume\n\nVolatility Correlation: Strong positive correlation (r = 0.78) between price volatility and trading volume - traders are more active during volatile market conditions\nDemographic Patterns:\n\nHighest activity in 25-34 age group (40%)\nSecond highest in 35-44 age group (30%)\nYounger traders (18-24) show preference for smaller-cap altcoins\n\nWeekend Effect: Trading volume increases by 35% on weekends compared to weekdays\n\n\n\n\nWhy I chose this visualization type:\nI used a dashboard-style multi-panel layout because: - It allows comparison of multiple related metrics simultaneously - Each panel tells a part of the overall story - Viewers can identify patterns across different dimensions of the data\nSpecific chart types: - Line chart for hourly trends: Best shows temporal patterns and peak hours - Pie chart for cryptocurrency distribution: Clearly shows proportional market share - Heatmap for correlations: Reveals relationships between variables at a glance - Stacked bar chart for demographics: Shows both total volume and age distribution\nColor scheme: - Blue gradient: Represents trust and stability (appropriate for financial data) - Orange/Red: Used for high volatility and peak activity (draws attention) - Green: Positive price movements and growth metrics - Consistent color palette across all panels for professional appearance\nLayout and formatting: - Clear axis labels with appropriate units (%, SGD, hours) - Legend placed consistently in top-right of each panel - Grid lines for easy value reading - Annotations highlighting key insights\n\n\n\nContext:\nThis project was completed for my DAT1001: Introduction to Data Analytics module during Semester 1, Year 1. The assignment required us to: - Identify a real-world dataset - Perform exploratory data analysis - Create meaningful visualizations - Present actionable insights\nChallenges I Faced:\n\nData Cleaning: The raw trading data had inconsistencies in timestamps and missing values (~5% of records). I used Python pandas to handle missing data through forward-fill for time-series continuity.\nChoosing the Right Visualization: Initially, I tried cramming everything into a single chart, which looked cluttered. After feedback from my professor, I redesigned it as a multi-panel dashboard.\nColor Accessibility: My first version used red-green colors, which I learned could be problematic for colorblind viewers. I switched to a blue-orange palette for better accessibility.\nPerformance: Rendering 50,000+ data points caused performance issues. I learned to aggregate data by hour instead of plotting every transaction.\n\nWhat I Learned:\n\nThe importance of data cleaning before visualization\nHow to choose chart types that match the data story\nThe power of multi-panel visualizations for complex datasets\nAccessibility considerations in data visualization design\nHow to use Python libraries effectively for data analysis\n\n\n\n\nTools Used: - Python 3.9: Main programming language - pandas: Data manipulation and aggregation - matplotlib & seaborn: Visualization libraries - numpy: Statistical calculations - Jupyter Notebook: Interactive development environment\nCode Snippet: ```python"
  },
  {
    "objectID": "data-visualization.html#title-of-your-visualization-project",
    "href": "data-visualization.html#title-of-your-visualization-project",
    "title": "Data Visualization",
    "section": "",
    "text": "[Describe what data you visualized and why this project was created]\n\n\n\n[Explain the dataset you worked with:] - What was the source? - What variables/metrics were included? - Time period covered - Size of the dataset\n\n\n\n\n\n\nYour Visualization\n\n\n[Or embed your visualization code if it‚Äôs interactive]\n\n\n\n[Explain what the visualization shows:] - Key metrics displayed - Trends or patterns visible - Important insights revealed - Any notable outliers or interesting findings\n\n\n\nWhy I chose this visualization type: - [Explain why you chose this particular chart/graph type] - [Discuss what makes it effective for this data]\nColor scheme: - [Explain your color choices and their meaning]\nLayout and formatting: - [Discuss axes, labels, legends, and other design elements]\n\n\n\n[Share the context:] - What course or project was this for? - What was the objective? - What challenges did you face? - What did you learn from creating this visualization?\n\n\n\nTools Used: - [List the software/libraries: Python (matplotlib, seaborn), R (ggplot2), Tableau, Excel, etc.]\nCode snippet (optional): ```python"
  },
  {
    "objectID": "hackathons.html",
    "href": "hackathons.html",
    "title": "Hackathon Experiences",
    "section": "",
    "text": "Throughout my academic journey, I‚Äôve participated in several hackathons that have shaped my problem-solving abilities and technical skills. Here are my key hackathon experiences:"
  },
  {
    "objectID": "hackathons.html#visa-fin-connect-hackathon-2024---runner-up",
    "href": "hackathons.html#visa-fin-connect-hackathon-2024---runner-up",
    "title": "Hackathon Experiences",
    "section": "VISA Fin-Connect Hackathon 2024 - Runner-Up ü•à",
    "text": "VISA Fin-Connect Hackathon 2024 - Runner-Up ü•à\n\nProject: Aura - AI-Powered Travel Rewards Platform\nAchievement: Secured Runner-Up position in a competitive national hackathon\nThe Problem: Traditional credit card rewards programs fail to personalize benefits and help users maximize their spending potential.\nOur Solution: We developed Aura, an innovative platform featuring:\n\n1. Spend Futures - AI Spending Predictor\n\nMachine learning model that analyzes historical spending patterns\nPredicts future expenses across different categories\nProvides personalized recommendations for optimal card usage\n\n\n\n2. Gamified Challenges\n\nInteractive challenges to encourage smart spending habits\nReward multipliers for achieving spending goals\nCompetitive leaderboards among friends\n\n\n\n3. Personalized Rewards Engine\n\nReal-time analysis of available credit card benefits\nSmart suggestions based on upcoming purchases\nIntegration with VISA‚Äôs rewards ecosystem\n\nTechnologies Used: Python, React, Node.js, TensorFlow, VISA APIs\nKey Takeaways: - Learned to develop under tight time constraints (24-48 hours) - Gained experience in FinTech API integration - Improved presentation and pitching skills - Understanding of the credit card rewards ecosystem"
  },
  {
    "objectID": "hackathons.html#hackrift-2025---finalist",
    "href": "hackathons.html#hackrift-2025---finalist",
    "title": "Hackathon Experiences",
    "section": "HackRift 2025 - Finalist üèÜ",
    "text": "HackRift 2025 - Finalist üèÜ\n\nProject: Hasslefree - AI Multilingual Voice-Powered Documentation App\nAchievement: Selected as one of the finalists among 100+ teams\nThe Problem: Healthcare documentation is time-consuming and often creates barriers for non-English speaking patients and doctors.\nOur Solution: Hasslefree - A full-stack application that revolutionizes medical documentation:\n\nCore Features\n1. Voice-to-Text Documentation - Real-time speech recognition using MediaRecorder API - Instant conversion of doctor-patient conversations to structured notes - Hands-free operation for busy healthcare professionals\n2. AI-Powered Intelligence - Integrated Grok API for intelligent summarization - Automatic extraction of key medical information - Context-aware suggestions and terminology correction\n3. Multilingual Support - Real-time translation capabilities - Support for multiple languages and dialects - Breaking language barriers in healthcare\n4. Edge Computing - Deployed on Edge Functions for ultra-low latency - Privacy-first architecture with local processing - HIPAA-compliant data handling\n\n\nTechnical Stack\n\nFrontend: Next.js, React\nBackend: Edge Functions (Vercel/Cloudflare)\nAI Integration: Grok API\nAudio Processing: MediaRecorder API, Web Speech API\nDeployment: Edge network for global accessibility\n\nKey Features: - Real-time voice transcription with &lt;1s latency - Multi-speaker detection and attribution - Automatic medical terminology correction - Export to standard medical documentation formats\nImpact: - Reduces documentation time by up to 70% - Improves accuracy in patient records - Enables better doctor-patient communication across language barriers - Scalable solution for healthcare facilities of all sizes\nWhat I Learned: - Edge computing architecture and deployment - Real-time audio processing and streaming - LLM API integration and prompt engineering - Healthcare technology compliance considerations - Building accessible and inclusive applications"
  },
  {
    "objectID": "hackathons.html#smart-nation-2.0-hackathon---peoples-choice-award",
    "href": "hackathons.html#smart-nation-2.0-hackathon---peoples-choice-award",
    "title": "Hackathon Experiences",
    "section": "Smart Nation 2.0 Hackathon - People‚Äôs Choice Award üåü",
    "text": "Smart Nation 2.0 Hackathon - People‚Äôs Choice Award üåü\n\nProject: Smart Preschool AI\nAchievement: Won the People‚Äôs Choice Award\nThe Problem: Preschools struggle with administrative tasks, parent communication, and tracking child development effectively.\nOur Solution: An AI-powered platform that streamlines preschool operations:\n\nAutomated attendance tracking\nAI-driven developmental milestone monitoring\nParent-teacher communication portal\nAdministrative task automation\n\nTechnologies Used: Python, React, Computer Vision, Mobile Development\nImpact: Our solution resonated with both judges and the public, earning the People‚Äôs Choice Award for its practical approach to solving real educational challenges."
  },
  {
    "objectID": "hackathons.html#lessons-from-hackathons",
    "href": "hackathons.html#lessons-from-hackathons",
    "title": "Hackathon Experiences",
    "section": "Lessons from Hackathons",
    "text": "Lessons from Hackathons\nParticipating in these hackathons has taught me:\n\nRapid Prototyping: Building functional MVPs in 24-48 hours\nTeam Collaboration: Working effectively under pressure with diverse teams\nUser-Centered Design: Always keeping the end-user in mind\nTechnical Versatility: Quickly learning and integrating new technologies\nPresentation Skills: Effectively communicating technical solutions to non-technical audiences\n\nThese experiences have not only enhanced my technical skills but also developed my ability to think creatively, work under pressure, and deliver solutions that address real-world problems."
  },
  {
    "objectID": "data-visualization.html#singapore-cryptocurrency-trading-patterns-analysis-2023-2024",
    "href": "data-visualization.html#singapore-cryptocurrency-trading-patterns-analysis-2023-2024",
    "title": "Data Visualization",
    "section": "",
    "text": "During my Data Analytics module in Year 1, I conducted an analysis of cryptocurrency trading patterns among Singapore investors. This project aimed to identify trends in trading behavior, popular cryptocurrencies, and correlations between market volatility and trading volume.\n\n\n\nDataset Details: - Source: Simulated trading data based on public cryptocurrency APIs (CoinGecko, Binance) - Variables: Trading volume, cryptocurrency type, time of day, price volatility, trader demographics - Time Period: January 2023 - December 2024 (24 months) - Size: 50,000+ transaction records from 1,200 simulated traders - Metrics: BTC, ETH, SOL, MATIC trading volumes, hourly price changes, trader age groups\n\n\n\n\n\n\nCryptocurrency Trading Patterns\n\n\nNote: This is a multi-panel visualization showing: - Top Left: Trading volume by hour of day (shows peak activity at 9-10 PM SGT) - Top Right: Most traded cryptocurrencies (Bitcoin dominates at 45%, followed by Ethereum at 30%) - Bottom Left: Correlation heatmap between volatility and trading volume - Bottom Right: Age group distribution of active traders\n\n\n\nKey Insights:\n\nPeak Trading Hours: Singapore crypto traders are most active between 9-11 PM, coinciding with evening relaxation time after work\nCryptocurrency Preferences:\n\nBitcoin: 45% of trading volume\nEthereum: 30% of trading volume\nSolana: 15% of trading volume\nOther altcoins: 10% of trading volume\n\nVolatility Correlation: Strong positive correlation (r = 0.78) between price volatility and trading volume - traders are more active during volatile market conditions\nDemographic Patterns:\n\nHighest activity in 25-34 age group (40%)\nSecond highest in 35-44 age group (30%)\nYounger traders (18-24) show preference for smaller-cap altcoins\n\nWeekend Effect: Trading volume increases by 35% on weekends compared to weekdays\n\n\n\n\nWhy I chose this visualization type:\nI used a dashboard-style multi-panel layout because: - It allows comparison of multiple related metrics simultaneously - Each panel tells a part of the overall story - Viewers can identify patterns across different dimensions of the data\nSpecific chart types: - Line chart for hourly trends: Best shows temporal patterns and peak hours - Pie chart for cryptocurrency distribution: Clearly shows proportional market share - Heatmap for correlations: Reveals relationships between variables at a glance - Stacked bar chart for demographics: Shows both total volume and age distribution\nColor scheme: - Blue gradient: Represents trust and stability (appropriate for financial data) - Orange/Red: Used for high volatility and peak activity (draws attention) - Green: Positive price movements and growth metrics - Consistent color palette across all panels for professional appearance\nLayout and formatting: - Clear axis labels with appropriate units (%, SGD, hours) - Legend placed consistently in top-right of each panel - Grid lines for easy value reading - Annotations highlighting key insights\n\n\n\nContext:\nThis project was completed for my DAT1001: Introduction to Data Analytics module during Semester 1, Year 1. The assignment required us to: - Identify a real-world dataset - Perform exploratory data analysis - Create meaningful visualizations - Present actionable insights\nChallenges I Faced:\n\nData Cleaning: The raw trading data had inconsistencies in timestamps and missing values (~5% of records). I used Python pandas to handle missing data through forward-fill for time-series continuity.\nChoosing the Right Visualization: Initially, I tried cramming everything into a single chart, which looked cluttered. After feedback from my professor, I redesigned it as a multi-panel dashboard.\nColor Accessibility: My first version used red-green colors, which I learned could be problematic for colorblind viewers. I switched to a blue-orange palette for better accessibility.\nPerformance: Rendering 50,000+ data points caused performance issues. I learned to aggregate data by hour instead of plotting every transaction.\n\nWhat I Learned:\n\nThe importance of data cleaning before visualization\nHow to choose chart types that match the data story\nThe power of multi-panel visualizations for complex datasets\nAccessibility considerations in data visualization design\nHow to use Python libraries effectively for data analysis\n\n\n\n\nTools Used: - Python 3.9: Main programming language - pandas: Data manipulation and aggregation - matplotlib & seaborn: Visualization libraries - numpy: Statistical calculations - Jupyter Notebook: Interactive development environment\nCode Snippet: ```python"
  }
]